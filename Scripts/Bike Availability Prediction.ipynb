{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92fomBVqP4xL"
   },
   "source": [
    "# Barcelona Bicycle Station Analysis and Prediction\n",
    "\n",
    "## Objective\n",
    "This notebook aims to analyze data from Barcelona's bicycle stations and predict the percentage of free docks at a given time based on historical data.\n",
    "\n",
    "## Data Sources\n",
    "We have two datasets:\n",
    "1. Historical data - Contains time-series information about station status\n",
    "2. Station data - Contains static information about each station\n",
    "\n",
    "We'll merge these datasets to create a comprehensive view for our analysis and prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhL-zBLbP4xM"
   },
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 21901,
     "status": "ok",
     "timestamp": 1742906278757,
     "user": {
      "displayName": "Pedro Corbelle Gomez Filho",
      "userId": "14478894779845786492"
     },
     "user_tz": -60
    },
    "id": "8qEFXy4SP4xN"
   },
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from scipy import stats\n",
    "\n",
    "# Date and time handling\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-laMvATP4xO"
   },
   "source": [
    "## 2. Load and Prepare Data\n",
    "\n",
    "### 2.1 Download Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 139348,
     "status": "ok",
     "timestamp": 1742906418108,
     "user": {
      "displayName": "Pedro Corbelle Gomez Filho",
      "userId": "14478894779845786492"
     },
     "user_tz": -60
    },
    "id": "8r5hYNv3P4xO"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# i2m = list(zip(range(1,13), ['Gener','Febrer','Marc','Abril','Maig','Juny','Juliol','Agost','Setembre','Octubre','Novembre','Desembre']))\n",
    "# for year in range(2024, 2019, -1):\n",
    "#     for month, month_name in i2m:\n",
    "#         if (month > 5) and (year>2023): continue\n",
    "#         print(f'curl -L -o \"{year}_{month:02d}_{month_name}_BicingNou_ESTACIONS.7z\" \"https://opendata-ajuntament.barcelona.cat/resources/bcn/BicingBCN/{year}_{month:02d}_{month_name}_BicingNou_ESTACIONS.7z\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SonzBQFZP4xQ"
   },
   "source": [
    "### 2.2 Load Station Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1742906418129,
     "user": {
      "displayName": "Pedro Corbelle Gomez Filho",
      "userId": "14478894779845786492"
     },
     "user_tz": -60
    },
    "id": "YY-0nAyeP4xQ",
    "outputId": "bbc96da6-ae8a-438a-b455-cb5218ea0aaa"
   },
   "outputs": [],
   "source": [
    "def load_station_data():\n",
    "    # Load station data or create sample data if file not found\n",
    "    try:\n",
    "        station_data = pd.read_csv('data/station_data/Informacio_Estacions_Bicing_2025.csv')\n",
    "        print(f\"Station data loaded successfully with {station_data.shape[0]} rows and {station_data.shape[1]} columns\")\n",
    "        return station_data\n",
    "    except FileNotFoundError:\n",
    "        print(\"Station data file not found. Creating sample data for demonstration...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhSEVCcOP4xR"
   },
   "source": [
    "### 2.3 Convert Timestamps and Handle Missing Values Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "aborted",
     "timestamp": 1742906418303,
     "user": {
      "displayName": "Pedro Corbelle Gomez Filho",
      "userId": "14478894779845786492"
     },
     "user_tz": -60
    },
    "id": "KIKyTI2pP4xS"
   },
   "outputs": [],
   "source": [
    "def prepare_historical_data(historical_data):\n",
    "    #Drop not needed columns\n",
    "    historical_data.drop(columns=['num_bikes_available_types.ebike',  'ttl',\n",
    "       'num_bikes_available_types.mechanical', 'num_bikes_available', 'last_updated'], inplace=True)\n",
    "\n",
    "    # Convert timestamp columns to datetime format\n",
    "    try:\n",
    "        historical_data['last_reported'] = pd.to_datetime(historical_data['last_reported'], unit='s')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting timestamps: {e}\")\n",
    "\n",
    "    # Handle missing values in historical data\n",
    "    numerical_cols = ['num_docks_available']\n",
    "    for col in numerical_cols:\n",
    "        if col in historical_data.columns and historical_data[col].isnull().sum() > 0:\n",
    "            historical_data[col].fillna(historical_data[col].median(), inplace=True)\n",
    "    return historical_data\n",
    "\n",
    "def prepare_station_data(station_data):\n",
    "\n",
    "    #Drop not needed columns\n",
    "    station_data.drop(columns=['name', 'address',\n",
    "       'cross_street', 'short_name', 'nearby_distance', '_ride_code_support', \n",
    "       'rental_uris', 'is_valet_station', 'physical_configuration', 'is_charging_station', 'post_code'], inplace=True)\n",
    "\n",
    "    # Handle missing values in station data\n",
    "    numerical_cols = ['lat', 'lon', 'altitude', 'capacity']\n",
    "    for col in numerical_cols:\n",
    "        if col in station_data.columns and station_data[col].isnull().sum() > 0:\n",
    "            station_data[col].fillna(station_data[col].median(), inplace=True)\n",
    "    return station_data    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejphsxxYP4xT"
   },
   "source": [
    "### 2.4 Create Hourly Average Dataset Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 161594,
     "status": "aborted",
     "timestamp": 1742906418307,
     "user": {
      "displayName": "Pedro Corbelle Gomez Filho",
      "userId": "14478894779845786492"
     },
     "user_tz": -60
    },
    "id": "XlnkiSZWP4xT"
   },
   "outputs": [],
   "source": [
    "# Create a new dataset with hourly averages of available bikes per station and date\n",
    "def create_hourly_avg_dataset(historical_data):\n",
    "\n",
    "    # Extract date components\n",
    "    historical_data['year'] = historical_data['last_reported'].dt.year\n",
    "    historical_data['month'] = historical_data['last_reported'].dt.month\n",
    "    historical_data['day'] = historical_data['last_reported'].dt.day\n",
    "    historical_data['hour'] = historical_data['last_reported'].dt.hour\n",
    "    historical_data['date'] = pd.to_datetime(historical_data[['year', 'month', 'day']])\n",
    "\n",
    "    # Group by station_id, year, month, day, hour and calculate average of num_bikes_available\n",
    "    hourly_avg = historical_data.groupby(['station_id', 'year', 'month', 'day', 'hour', 'date'])['num_docks_available'].mean().reset_index()\n",
    "\n",
    "    return hourly_avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Merge data Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data with station data on station_id\n",
    "def merge_data(data,station_data):\n",
    "    merged_data = pd.merge(data, station_data, on='station_id', how='inner')\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFL3qZOEP4xS"
   },
   "source": [
    "### 2.6 Feature Engineering Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 161593,
     "status": "aborted",
     "timestamp": 1742906418310,
     "user": {
      "displayName": "Pedro Corbelle Gomez Filho",
      "userId": "14478894779845786492"
     },
     "user_tz": -60
    },
    "id": "PbKNBPwTP4xS"
   },
   "outputs": [],
   "source": [
    "def create_additional_features(data):\n",
    "    # Add time-based features to historical data\n",
    "    data['day_of_week'] = data['date'].dt.dayofweek\n",
    "    data['is_weekend'] = data['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "    # Add distance from city center for each station\n",
    "    # Barcelona city center coordinates (Plaça de Catalunya)\n",
    "    barcelona_center_lat = 41.3874\n",
    "    barcelona_center_lon = 2.1686\n",
    "\n",
    "    if all(col in data.columns for col in ['lat', 'lon']):\n",
    "        # Calculate Haversine distance (in kilometers)\n",
    "        def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "            # Convert decimal degrees to radians\n",
    "            lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "            # Haversine formula\n",
    "            dlon = lon2 - lon1\n",
    "            dlat = lat2 - lat1\n",
    "            a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "            c = 2 * np.arcsin(np.sqrt(a))\n",
    "            r = 6371  # Radius of Earth in kilometers\n",
    "            return c * r\n",
    "\n",
    "        data['distance_from_center'] = data.apply(\n",
    "            lambda row: haversine_distance(row['lat'], row['lon'], barcelona_center_lat, barcelona_center_lon),\n",
    "            axis=1\n",
    "        ).round(3)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def create_free_docks_percentage(data):\n",
    "    # Calculate the percentage of free docks (our target variable)\n",
    "    data[f'free_docks_percentage'] = (data[f'num_docks_available'] / data['capacity']).round(2)\n",
    "    return data\n",
    "\n",
    "def drop_other_features(data):\n",
    "    data.drop(columns=['num_docks_available', 'date'], inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Create final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station data loaded successfully with 516 rows and 16 columns\n",
      "File 2020_01_Gener_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 274832 rows.\n",
      "File 2020_02_Febrer_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 284373 rows.\n",
      "File 2020_03_Marc_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 162924 rows.\n",
      "File 2020_04_Abril_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 101422 rows.\n",
      "File 2020_05_Maig_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 308279 rows.\n",
      "File 2020_06_Juny_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 339627 rows.\n",
      "File 2020_07_Juliol_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 357907 rows.\n",
      "File 2020_08_Agost_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 320242 rows.\n",
      "File 2020_09_Setembre_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 350666 rows.\n",
      "File 2020_10_Octubre_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 368375 rows.\n",
      "File 2020_11_Novembre_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 304949 rows.\n",
      "File 2020_12_Desembre_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 366402 rows.\n",
      "File 2021_01_Gener_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 372778 rows.\n",
      "File 2021_02_Febrer_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 332338 rows.\n",
      "File 2021_03_Març_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 373967 rows.\n",
      "File 2021_04_Abril_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 362361 rows.\n",
      "File 2021_05_Maig_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 373524 rows.\n",
      "File 2021_06_Juny_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 355972 rows.\n",
      "File 2021_07_Juliol_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 367307 rows.\n",
      "File 2021_08_Agost_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 372953 rows.\n",
      "File 2021_09_Setembre_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 362175 rows.\n",
      "File 2021_10_Octubre_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 373746 rows.\n",
      "File 2021_11_Novembre_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 357739 rows.\n",
      "File 2021_12_Desembre_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 365576 rows.\n",
      "File 2022_01_Gener_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 365290 rows.\n",
      "File 2022_02_Febrer_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 338520 rows.\n",
      "File 2022_03_Març_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 375168 rows.\n",
      "File 2022_04_Abril_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 363008 rows.\n",
      "File 2022_05_Maig_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 375001 rows.\n",
      "File 2022_06_Juny_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 362641 rows.\n",
      "File 2022_07_Juliol_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 374103 rows.\n",
      "File 2022_08_Agost_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 372405 rows.\n",
      "File 2022_09_Setembre_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 360901 rows.\n",
      "File 2022_10_Octubre_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 373287 rows.\n",
      "File 2022_11_Novembre_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 359388 rows.\n",
      "File 2022_12_Desembre_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 373131 rows.\n",
      "File 2023_01_Gener_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 336728 rows.\n",
      "File 2023_02_Febrer_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 335934 rows.\n",
      "File 2023_03_Marc_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 356033 rows.\n",
      "File 2023_04_Abril_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 300531 rows.\n",
      "File 2023_05_Maig_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 352417 rows.\n",
      "File 2023_06_Juny_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 352523 rows.\n",
      "File 2023_07_Juliol_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 376210 rows.\n",
      "File 2023_08_Agost_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 258180 rows.\n",
      "File 2023_09_Setembre_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 309275 rows.\n",
      "File 2023_10_Octubre_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 371999 rows.\n",
      "File 2023_11_Novembre_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 354202 rows.\n",
      "File 2023_12_Desembre_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 374199 rows.\n",
      "File 2024_01_Gener_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 362703 rows.\n",
      "File 2024_02_Febrer_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 352019 rows.\n",
      "File 2024_03_Marc_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 374947 rows.\n",
      "File 2024_04_Abril_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 363572 rows.\n",
      "File 2024_05_Maig_BicingNou_ESTACIONS.csv started!\n",
      "File loaded successfully with 375113 rows.\n"
     ]
    }
   ],
   "source": [
    "station_data = load_station_data()\n",
    "station_data = prepare_station_data(station_data)\n",
    "\n",
    "csv_files = [f for f in os.listdir(\"./data\") if f.endswith('.csv')]\n",
    "\n",
    "for file in csv_files:\n",
    "    print(f'File {file} started!')\n",
    "    file_path = os.path.join(\"./data\", file)\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    data = prepare_historical_data(data)\n",
    "    data = create_hourly_avg_dataset(data)\n",
    "    data = merge_data(data, station_data)\n",
    "    data = create_additional_features(data)\n",
    "    data = create_free_docks_percentage(data)\n",
    "    data = drop_other_features(data)\n",
    "\n",
    "    write_header = not os.path.exists(\"./data/historical_data.csv\")\n",
    "    data.to_csv(\"./data/historical_data.csv\", mode='a', index=False, header=write_header)\n",
    "    print(f\"File loaded successfully with {data.shape[0]} rows.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICPJhkriP4xS"
   },
   "source": [
    "## 3. Load complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 161592,
     "status": "aborted",
     "timestamp": 1742906418312,
     "user": {
      "displayName": "Pedro Corbelle Gomez Filho",
      "userId": "14478894779845786492"
     },
     "user_tz": -60
    },
    "id": "Fl29LGSzP4xT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final data loaded successfully with 18209862 rows and 13 columns\n"
     ]
    }
   ],
   "source": [
    "# Load final dataset\n",
    "try:\n",
    "    complete_data = pd.read_csv('data/historical_data.csv')\n",
    "    print(f\"Final data loaded successfully with {complete_data.shape[0]} rows and {complete_data.shape[1]} columns\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Station data file not found. Creating sample data for demonstration...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (18209862, 13)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18209862 entries, 0 to 18209861\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   station_id             float64\n",
      " 1   year                   float64\n",
      " 2   month                  float64\n",
      " 3   day                    float64\n",
      " 4   hour                   float64\n",
      " 5   lat                    float64\n",
      " 6   lon                    float64\n",
      " 7   altitude               float64\n",
      " 8   capacity               int64  \n",
      " 9   day_of_week            int64  \n",
      " 10  is_weekend             int64  \n",
      " 11  distance_from_center   float64\n",
      " 12  free_docks_percentage  float64\n",
      "dtypes: float64(10), int64(3)\n",
      "memory usage: 1.8 GB\n",
      "None\n",
      "Empty DataFrame\n",
      "Columns: [Missing Values, Percentage]\n",
      "Index: []\n",
      "Total records with missing lat/lon: 0\n",
      "Station IDs with missing location data: []\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# Basic Info\n",
    "# ========================\n",
    "print(\"Shape:\", complete_data.shape)\n",
    "print(complete_data.info())\n",
    "\n",
    "# ========================\n",
    "# Missing Values Analysis\n",
    "# ========================\n",
    "missing_counts = complete_data.isnull().sum()\n",
    "missing_percentage = (missing_counts / len(complete_data)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({'Missing Values': missing_counts, 'Percentage': missing_percentage})\n",
    "print(missing_df[missing_df['Missing Values'] > 0].sort_values(by='Percentage', ascending=False))\n",
    "\n",
    "# Filter rows where lat or lon is missing\n",
    "missing_location = complete_data[complete_data[['lat', 'lon']].isnull().any(axis=1)]\n",
    "\n",
    "# Check which station_ids have missing location data\n",
    "stations_with_missing_location = missing_location['station_id'].unique()\n",
    "\n",
    "print(f\"Total records with missing lat/lon: {missing_location.shape[0]}\")\n",
    "print(f\"Station IDs with missing location data: {stations_with_missing_location}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zuO1-KZP4xU"
   },
   "source": [
    "## 5. Create LAG Features for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = complete_data.sort_values(by=['station_id', 'year', 'month', 'day', 'hour'])\n",
    "\n",
    "# Create lag features\n",
    "for lag in range(1, 5):\n",
    "    complete_data[f'ctx-{lag}'] = complete_data.groupby('station_id')['free_docks_percentage'].shift(lag)\n",
    "\n",
    "ctx_columns = ['ctx-1', 'ctx-2', 'ctx-3', 'ctx-4']\n",
    "\n",
    "# Drop rows where any of these columns is NaN\n",
    "complete_data = complete_data.dropna(subset=ctx_columns).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Split Data and Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Prepare features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 161593,
     "status": "aborted",
     "timestamp": 1742906418315,
     "user": {
      "displayName": "Pedro Corbelle Gomez Filho",
      "userId": "14478894779845786492"
     },
     "user_tz": -60
    },
    "id": "pmNSF4BeP4xU"
   },
   "outputs": [],
   "source": [
    "# Define the target\n",
    "target = 'free_docks_percentage'\n",
    "\n",
    "# Define the features\n",
    "features = ['station_id', 'year', 'month', 'day', 'hour', 'lat', 'lon', 'altitude',\n",
    "            'capacity', 'day_of_week', 'is_weekend', 'distance_from_center',\n",
    "            'ctx-1', 'ctx-2', 'ctx-3', 'ctx-4']\n",
    "\n",
    "X = complete_data[features]\n",
    "y = complete_data[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split: Train + Val and Test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Second split: Train and Val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42, shuffle=True)\n",
    "# Result: 60% train, 20% val, 20% test\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "val_data = lgb.Dataset(X_val, label=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-27 12:40:31,054] A new study created in memory with name: no-name-fa53e3f1-720a-4d53-810f-fb23a29d3714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 0.102175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-27 12:41:34,492] Trial 0 finished with value: 0.010439681553858795 and parameters: {'learning_rate': 0.048989029148989234, 'num_leaves': 60, 'max_depth': 6, 'min_data_in_leaf': 99, 'feature_fraction': 0.9805615770217293, 'bagging_fraction': 0.6178382985419547, 'bagging_freq': 7}. Best is trial 0 with value: 0.010439681553858795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 0.0972207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-27 12:43:16,933] Trial 1 finished with value: 0.009451865619860632 and parameters: {'learning_rate': 0.25228929520822735, 'num_leaves': 246, 'max_depth': 8, 'min_data_in_leaf': 29, 'feature_fraction': 0.8385602398609304, 'bagging_fraction': 0.9382375139085373, 'bagging_freq': 3}. Best is trial 1 with value: 0.009451865619860632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 0.0978452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-27 12:44:40,107] Trial 2 finished with value: 0.009573690359003807 and parameters: {'learning_rate': 0.26105189214136887, 'num_leaves': 102, 'max_depth': 12, 'min_data_in_leaf': 28, 'feature_fraction': 0.7048650554462162, 'bagging_fraction': 0.974682788193044, 'bagging_freq': 5}. Best is trial 1 with value: 0.009451865619860632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 0.10388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-27 12:45:17,255] Trial 3 finished with value: 0.010791071908650654 and parameters: {'learning_rate': 0.14477568034990967, 'num_leaves': 110, 'max_depth': 3, 'min_data_in_leaf': 65, 'feature_fraction': 0.9782650560683331, 'bagging_fraction': 0.8488729497119614, 'bagging_freq': 5}. Best is trial 1 with value: 0.009451865619860632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 0.101494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-27 12:46:09,783] Trial 4 finished with value: 0.010301073048720093 and parameters: {'learning_rate': 0.10609129899227325, 'num_leaves': 64, 'max_depth': 5, 'min_data_in_leaf': 40, 'feature_fraction': 0.9555356022099586, 'bagging_fraction': 0.7381736539304262, 'bagging_freq': 4}. Best is trial 1 with value: 0.009451865619860632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 0.0993169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-27 12:47:10,423] Trial 5 finished with value: 0.009863846401999023 and parameters: {'learning_rate': 0.17001191631040183, 'num_leaves': 50, 'max_depth': 9, 'min_data_in_leaf': 64, 'feature_fraction': 0.8267772569553107, 'bagging_fraction': 0.9222370822210126, 'bagging_freq': 6}. Best is trial 1 with value: 0.009451865619860632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 0.101006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-27 12:48:18,192] Trial 6 finished with value: 0.010202143510255045 and parameters: {'learning_rate': 0.06573723133176149, 'num_leaves': 61, 'max_depth': 8, 'min_data_in_leaf': 83, 'feature_fraction': 0.7244351397751843, 'bagging_fraction': 0.9855704790181153, 'bagging_freq': 1}. Best is trial 1 with value: 0.009451865619860632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 0.100561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-27 12:49:08,834] Trial 7 finished with value: 0.010112560534811207 and parameters: {'learning_rate': 0.18526901926499664, 'num_leaves': 290, 'max_depth': 5, 'min_data_in_leaf': 43, 'feature_fraction': 0.9412313135512638, 'bagging_fraction': 0.9253770664852209, 'bagging_freq': 6}. Best is trial 1 with value: 0.009451865619860632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 0.103289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-27 12:50:38,837] Trial 8 finished with value: 0.010668542743839849 and parameters: {'learning_rate': 0.02996591290441218, 'num_leaves': 167, 'max_depth': 8, 'min_data_in_leaf': 78, 'feature_fraction': 0.7944998054495347, 'bagging_fraction': 0.7546107134776684, 'bagging_freq': 6}. Best is trial 1 with value: 0.009451865619860632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 0.0980032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-27 12:51:38,166] Trial 9 finished with value: 0.009604620267305944 and parameters: {'learning_rate': 0.281216136367123, 'num_leaves': 121, 'max_depth': 11, 'min_data_in_leaf': 83, 'feature_fraction': 0.6287564718110673, 'bagging_fraction': 0.9046432642365693, 'bagging_freq': 1}. Best is trial 1 with value: 0.009451865619860632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.25228929520822735, 'num_leaves': 246, 'max_depth': 8, 'min_data_in_leaf': 29, 'feature_fraction': 0.8385602398609304, 'bagging_fraction': 0.9382375139085373, 'bagging_freq': 3}\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's rmse: 0.0985685\n",
      "[100]\tvalid_0's rmse: 0.0972207\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 0.0972207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x1bc61d2fd10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'feature_pre_filter': False,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "    }\n",
    "    model = lgb.train(params, train_data, valid_sets=[val_data], callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(0)])\n",
    "    preds = model.predict(X_val)\n",
    "    rmse = mean_squared_error(y_val, preds)\n",
    "    return rmse\n",
    "\n",
    "# Run tuning\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Best parameters\n",
    "best_params = study.best_trial.params\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# ========================\n",
    "# 4. Train final model\n",
    "# ========================\n",
    "final_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'verbosity': -1,\n",
    "    'boosting_type': 'gbdt',\n",
    "    **best_params\n",
    "}\n",
    "\n",
    "model = lgb.train(\n",
    "    final_params,\n",
    "    train_data,\n",
    "    valid_sets=[val_data],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(50)]\n",
    ")\n",
    "model.save_model('lightgbm_free_docks_model.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAbso1BNP4xU"
   },
   "source": [
    "## 7. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 161594,
     "status": "aborted",
     "timestamp": 1742906418317,
     "user": {
      "displayName": "Pedro Corbelle Gomez Filho",
      "userId": "14478894779845786492"
     },
     "user_tz": -60
    },
    "id": "RiMx62gzP4xU"
   },
   "outputs": [],
   "source": [
    "# Define evaluation function\n",
    "def evaluate_model(model, X, y, set_name):\n",
    "    y_pred = model.predict(X)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "\n",
    "    print(f\"{set_name} set metrics:\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    print(f\"  R²: {r2:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 161595,
     "status": "aborted",
     "timestamp": 1742906418319,
     "user": {
      "displayName": "Pedro Corbelle Gomez Filho",
      "userId": "14478894779845786492"
     },
     "user_tz": -60
    },
    "id": "1rz-gD6rP4xU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set metrics:\n",
      "  MSE: 0.0094\n",
      "  RMSE: 0.0970\n",
      "  MAE: 0.0626\n",
      "  R²: 0.8768\n",
      "Validation set metrics:\n",
      "  MSE: 0.0095\n",
      "  RMSE: 0.0972\n",
      "  MAE: 0.0628\n",
      "  R²: 0.8762\n",
      "Test set metrics:\n",
      "  MSE: 0.0095\n",
      "  RMSE: 0.0973\n",
      "  MAE: 0.0628\n",
      "  R²: 0.8760\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mse': 0.009471682580178659,\n",
       " 'rmse': 0.09732256973682239,\n",
       " 'mae': 0.06284088405545417,\n",
       " 'r2': 0.8759560213790335}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model, X_train, y_train, \"Train\")\n",
    "evaluate_model(model, X_val, y_val, \"Validation\")\n",
    "evaluate_model(model, X_test, y_test, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Prepare Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Load Metadata Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station data loaded successfully with 401511 rows and 9 columns\n"
     ]
    }
   ],
   "source": [
    "def load_submission_metadata():\n",
    "    # Load metadata\n",
    "    try:\n",
    "        submission_data = pd.read_csv('sample/metadata_sample_submission_2025.csv')\n",
    "        print(f\"Station data loaded successfully with {submission_data.shape[0]} rows and {submission_data.shape[1]} columns\")\n",
    "        return submission_data\n",
    "    except FileNotFoundError:\n",
    "        print(\"Station data file not found. Creating sample data for demonstration...\")\n",
    "\n",
    "submission_data = load_submission_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Prepare Subsmission Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station data loaded successfully with 516 rows and 16 columns\n",
      "Columns match: True\n"
     ]
    }
   ],
   "source": [
    "station_data = load_station_data()\n",
    "station_data = prepare_station_data(station_data)\n",
    "\n",
    "submission_data = merge_data(submission_data, station_data)\n",
    "submission_data['year'] = 2024\n",
    "submission_data['date'] = pd.to_datetime(submission_data[['year', 'month', 'day']])\n",
    "submission_data = create_additional_features(submission_data)\n",
    "submission_data = submission_data.drop(columns=['index','date'])\n",
    "\n",
    "submission_data = submission_data[X_train.columns]\n",
    "\n",
    "# Check if the columns of prediction_data match the columns of X_train\n",
    "columns_match = submission_data.columns.tolist() == X_train.columns.tolist()\n",
    "# Print if the columns match\n",
    "print(\"Columns match:\", columns_match)\n",
    "\n",
    "# If the columns don't match, print the differences\n",
    "if not columns_match:\n",
    "    print(\"Columns in prediction_data that are not in X_train:\", set(submission_data.columns) - set(X_train.columns))\n",
    "    print(\"Columns in X_train that are not in prediction_data:\", set(X_train.columns) - set(submission_data.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>altitude</th>\n",
       "      <th>capacity</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>distance_from_center</th>\n",
       "      <th>ctx-1</th>\n",
       "      <th>ctx-2</th>\n",
       "      <th>ctx-3</th>\n",
       "      <th>ctx-4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>41.397978</td>\n",
       "      <td>2.180107</td>\n",
       "      <td>16.0</td>\n",
       "      <td>46</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.518</td>\n",
       "      <td>0.311594</td>\n",
       "      <td>0.324275</td>\n",
       "      <td>0.378623</td>\n",
       "      <td>0.490942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>41.397978</td>\n",
       "      <td>2.180107</td>\n",
       "      <td>16.0</td>\n",
       "      <td>46</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.518</td>\n",
       "      <td>0.394928</td>\n",
       "      <td>0.346014</td>\n",
       "      <td>0.311594</td>\n",
       "      <td>0.271739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>41.397978</td>\n",
       "      <td>2.180107</td>\n",
       "      <td>16.0</td>\n",
       "      <td>46</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.518</td>\n",
       "      <td>0.721014</td>\n",
       "      <td>0.697464</td>\n",
       "      <td>0.650362</td>\n",
       "      <td>0.538043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>41.397978</td>\n",
       "      <td>2.180107</td>\n",
       "      <td>16.0</td>\n",
       "      <td>46</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.518</td>\n",
       "      <td>0.807971</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.800725</td>\n",
       "      <td>0.789855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>41.397978</td>\n",
       "      <td>2.180107</td>\n",
       "      <td>16.0</td>\n",
       "      <td>46</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.518</td>\n",
       "      <td>0.793478</td>\n",
       "      <td>0.817029</td>\n",
       "      <td>0.871377</td>\n",
       "      <td>0.860507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_id  year  month  day  hour        lat       lon  altitude  \\\n",
       "0           1  2024      6    1     3  41.397978  2.180107      16.0   \n",
       "1           1  2024      6    1     8  41.397978  2.180107      16.0   \n",
       "2           1  2024      6    1    13  41.397978  2.180107      16.0   \n",
       "3           1  2024      6    1    18  41.397978  2.180107      16.0   \n",
       "4           1  2024      6    1    23  41.397978  2.180107      16.0   \n",
       "\n",
       "   capacity  day_of_week  is_weekend  distance_from_center     ctx-1  \\\n",
       "0        46            5           1                 1.518  0.311594   \n",
       "1        46            5           1                 1.518  0.394928   \n",
       "2        46            5           1                 1.518  0.721014   \n",
       "3        46            5           1                 1.518  0.807971   \n",
       "4        46            5           1                 1.518  0.793478   \n",
       "\n",
       "      ctx-2     ctx-3     ctx-4  \n",
       "0  0.324275  0.378623  0.490942  \n",
       "1  0.346014  0.311594  0.271739  \n",
       "2  0.697464  0.650362  0.538043  \n",
       "3  0.791667  0.800725  0.789855  \n",
       "4  0.817029  0.871377  0.860507  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(submission_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final submission dataframe\n",
    "final_submission = pd.DataFrame({'percentage_docks_available': predictions})\n",
    "\n",
    "# Adding index column to final submission, starting at 0\n",
    "final_submission['index'] = range(len(submission_data))\n",
    "\n",
    "# Reorder columns to put index first\n",
    "final_submission = final_submission[['index', 'percentage_docks_available']]\n",
    "final_submission.to_csv(\"./submission_data.csv\", mode='a', index=False, header=True)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
